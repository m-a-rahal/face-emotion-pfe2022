{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/m-a-rahal/face-emotion-pfe2022/blob/main/test_2022_certitude.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2gbfD4dHQ_k"
      },
      "outputs": [],
      "source": [
        "# !python \"/content/Deep-Emotion/main.py\" -"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhUAgWry-X4G"
      },
      "source": [
        "# Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqSK2UBVBxQm"
      },
      "source": [
        "## Mount drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1c04KUHq5f4p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09101e71-8e84-4baf-9df0-97e621865eeb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "zwEpdbA0KSn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf4bHowxLdy7"
      },
      "source": [
        "# Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ft1p8dckCieP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class Deep_Emotion_VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Deep_Emotion class contains the network architecture.\n",
        "        '''\n",
        "        super(Deep_Emotion_VGG16,self).__init__()\n",
        "        self.vgg16_conv1_1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.vgg16_conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.vgg16_conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.vgg16_conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.vgg16_conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.vgg16_maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.norm = nn.BatchNorm2d(10) # TODO: increase the features here or remove this layer completeley\n",
        "        \n",
        "        # TODO: try the old linear networks, maybe they perform better\n",
        "        self.vgg16_fc1 = nn.Linear(2816, 512) #RuntimeError: mat1 and mat2 shapes cannot be multiplied (64x512 and 25088x4096)\n",
        "        self.vgg16_fc2 = nn.Linear(512, 512)\n",
        "        self.vgg16_fc3 = nn.Linear(512, 7)\n",
        "\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(640, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "        )\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, 640)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "    def forward(self,input):\n",
        "        x1 = self.stn(input)\n",
        "        x2 = self.vgg16_forward(input)\n",
        "        x1 = torch.flatten(x1, start_dim=1)\n",
        "        x2 = torch.flatten(x2, start_dim=1)\n",
        "        # concat\n",
        "        x = torch.concat([x1,x2], dim=1)\n",
        "\n",
        "        # x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.vgg16_fc1(x))\n",
        "        x = F.dropout(x, 0.5) #dropout was included to combat overfitting\n",
        "        x = F.relu(self.vgg16_fc2(x))\n",
        "        x = F.dropout(x, 0.5)\n",
        "        x = self.vgg16_fc3(x)\n",
        "        # NOTE: DO NOT ADD SOFTMAX AT THE END, here's why :  https://stackoverflow.com/questions/67466531/cnn-pytorch-only-batches-of-spatial-targets-supported-error\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def vgg16_forward(self, x):\n",
        "        x = F.relu(self.vgg16_conv1_1(x))\n",
        "        x = F.relu(self.vgg16_conv1_2(x))\n",
        "        x = self.vgg16_maxpool(x)\n",
        "        x = F.relu(self.vgg16_conv2_1(x))\n",
        "        x = F.relu(self.vgg16_conv2_2(x))\n",
        "        x = self.vgg16_maxpool(x)\n",
        "        x = F.relu(self.vgg16_conv3_1(x))\n",
        "        x = F.relu(self.vgg16_conv3_2(x))\n",
        "        x = F.relu(self.vgg16_conv3_3(x))\n",
        "        x = self.vgg16_maxpool(x)\n",
        "        x = F.relu(self.vgg16_conv4_1(x))\n",
        "        x = F.relu(self.vgg16_conv4_2(x))\n",
        "        x = F.relu(self.vgg16_conv4_3(x))\n",
        "        x = self.vgg16_maxpool(x)\n",
        "        x = F.relu(self.vgg16_conv5_1(x))\n",
        "        x = F.relu(self.vgg16_conv5_2(x))\n",
        "        x = F.relu(self.vgg16_conv5_3(x))\n",
        "        x = self.vgg16_maxpool(x)\n",
        "        return x\n",
        "\n",
        "print(\"Model archticture: \", Deep_Emotion_VGG16())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLg4KqbR4bkX"
      },
      "outputs": [],
      "source": [
        "class Simple_VGG16(nn.Module):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Deep_Emotion class contains the network architecture.\n",
        "        '''\n",
        "        super(Simple_VGG16,self).__init__()\n",
        "        self.vgg16_conv1_1 = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv1_2 = nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=1)\n",
        "\n",
        "        self.vgg16_conv2_1 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv2_2 = nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, padding=1)\n",
        "\n",
        "        self.vgg16_conv3_1 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv3_2 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv3_3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, padding=1)\n",
        "\n",
        "        self.vgg16_conv4_1 = nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv4_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv4_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.vgg16_conv5_1 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv5_2 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "        self.vgg16_conv5_3 = nn.Conv2d(in_channels=512, out_channels=512, kernel_size=3, padding=1)\n",
        "\n",
        "        self.vgg16_maxpool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.norm = nn.BatchNorm2d(10) # TODO: increase the features here or remove this layer completeley\n",
        "        \n",
        "        # TODO: try the old linear networks, maybe they perform better\n",
        "        self.vgg16_fc1 = nn.Linear(512, 512)\n",
        "        self.vgg16_fc2 = nn.Linear(512, 128)\n",
        "        self.vgg16_fc3 = nn.Linear(128, 7)\n",
        "\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(640, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "        )\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, 640)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "    def forward(self,input):\n",
        "        #x1 = self.stn(input)\n",
        "        x = self.vgg16_forward(input)\n",
        "        x = torch.flatten(x, start_dim=1)\n",
        "        #x = torch.flatten(x, start_dim=1)\n",
        "        x = F.relu(self.vgg16_fc1(x))\n",
        "        x = F.dropout(x, 0.5) #dropout was included to combat overfitting\n",
        "        x = F.relu(self.vgg16_fc2(x))\n",
        "        x = F.dropout(x, 0.5)\n",
        "        x = self.vgg16_fc3(x)\n",
        "        # NOTE: DO NOT ADD SOFTMAX AT THE END, here's why :  https://stackoverflow.com/questions/67466531/cnn-pytorch-only-batches-of-spatial-targets-supported-error\n",
        "        \n",
        "        return x\n",
        "\n",
        "    def vgg16_forward(self, x):\n",
        "        x = F.relu(self.vgg16_conv1_1(x))\n",
        "        x = F.relu(self.vgg16_conv1_2(x))\n",
        "        x = self.vgg16_maxpool(x)\n",
        "        x = F.relu(self.vgg16_conv2_1(x))\n",
        "        x = F.relu(self.vgg16_conv2_2(x))\n",
        "        x = self.vgg16_maxpool(x)\n",
        "        x = F.relu(self.vgg16_conv3_1(x))\n",
        "        x = F.relu(self.vgg16_conv3_2(x))\n",
        "        x = F.relu(self.vgg16_conv3_3(x))\n",
        "        x = self.vgg16_maxpool(x)\n",
        "        x = F.relu(self.vgg16_conv4_1(x))\n",
        "        x = F.relu(self.vgg16_conv4_2(x))\n",
        "        x = F.relu(self.vgg16_conv4_3(x))\n",
        "        x = self.vgg16_maxpool(x)\n",
        "        x = F.relu(self.vgg16_conv5_1(x))\n",
        "        x = F.relu(self.vgg16_conv5_2(x))\n",
        "        x = F.relu(self.vgg16_conv5_3(x))\n",
        "        x = self.vgg16_maxpool(x)\n",
        "        return x\n",
        "\n",
        "print(\"Model archticture: \", Deep_Emotion_VGG16())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QT_Z1l-Ooh0A"
      },
      "outputs": [],
      "source": [
        "class Deep_Emotion(nn.Module):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Deep_Emotion class contains the network architecture.\n",
        "        '''\n",
        "        super(Deep_Emotion,self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,10,3)\n",
        "        self.conv2 = nn.Conv2d(10,10,3)\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(10,10,3)\n",
        "        self.conv4 = nn.Conv2d(10,10,3)\n",
        "        self.pool4 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.norm = nn.BatchNorm2d(10)\n",
        "\n",
        "        self.fc1 = nn.Linear(810,50)\n",
        "        self.fc2 = nn.Linear(50,7)\n",
        "\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(640, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "        )\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, 640)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "    def forward(self,input):\n",
        "        out = self.stn(input)\n",
        "\n",
        "        out = F.relu(self.conv1(out))\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(self.pool2(out))\n",
        "\n",
        "        out = F.relu(self.conv3(out))\n",
        "        out = self.norm(self.conv4(out))\n",
        "        out = F.relu(self.pool4(out))\n",
        "\n",
        "        out = F.dropout(out)\n",
        "        out = out.view(-1, 810)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dHQanrj69on"
      },
      "outputs": [],
      "source": [
        "class Shallow_Emotion(nn.Module):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Deep_Emotion class contains the network architecture.\n",
        "        '''\n",
        "        super(Shallow_Emotion,self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,10,3)\n",
        "        self.conv2 = nn.Conv2d(10,10,3)\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(10,10,3)\n",
        "        self.conv4 = nn.Conv2d(10,10,3)\n",
        "        self.pool4 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        # self.norm = nn.BatchNorm2d(10)\n",
        "\n",
        "        self.fc1 = nn.Linear(810,50)\n",
        "        self.fc2 = nn.Linear(50,7)\n",
        "\n",
        "    def forward(self,input):\n",
        "        out = F.relu(self.conv1(input))\n",
        "        out = self.conv2(out)\n",
        "        out = F.relu(self.pool2(out))\n",
        "\n",
        "        out = F.relu(self.conv3(out))\n",
        "        out = self.conv4(out)\n",
        "        out = F.relu(self.pool4(out))\n",
        "\n",
        "        out = F.dropout(out)\n",
        "        out = out.view(-1, 810)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VOzUi_8tgGQw"
      },
      "outputs": [],
      "source": [
        "class Deep_Emotion_with_concat(nn.Module):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Deep_Emotion class contains the network architecture.\n",
        "        '''\n",
        "        super(Deep_Emotion_with_concat,self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,10,3)\n",
        "        self.conv2 = nn.Conv2d(10,10,3)\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(10,10,3)\n",
        "        self.conv4 = nn.Conv2d(10,10,3)\n",
        "        self.pool4 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.norm = nn.BatchNorm2d(10)\n",
        "\n",
        "        self.fc1 = nn.Linear(3114,1024) #mat1 and mat2 shapes cannot be multiplied (128x3114 and 810x50)\n",
        "        self.fc2 = nn.Linear(1024,7)\n",
        "\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(640, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "        )\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, 640)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "    def forward(self,input):\n",
        "        x1 = self.stn(input)\n",
        "\n",
        "        x2 = F.relu(self.conv1(input))\n",
        "        x2 = self.conv2(x2)\n",
        "        x2 = F.relu(self.pool2(x2))\n",
        "\n",
        "        x2 = F.relu(self.conv3(x2))\n",
        "        x2 = self.norm(self.conv4(x2))\n",
        "        x2 = F.relu(self.pool4(x2))\n",
        "\n",
        "        x2 = F.dropout(x2)\n",
        "        x2 = x2.view(-1, 810)\n",
        "        \n",
        "        x1 = torch.flatten(x1, start_dim=1)\n",
        "        out = torch.concat([x1,x2], dim=1)\n",
        "        \n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Deep_Emotion_with_mult(nn.Module):\n",
        "    def __init__(self):\n",
        "        '''\n",
        "        Deep_Emotion class contains the network architecture.\n",
        "        '''\n",
        "        super(Deep_Emotion_with_mult,self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1,10,3)\n",
        "        self.conv2 = nn.Conv2d(10,10,3)\n",
        "        self.pool2 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(10,10,3)\n",
        "        self.conv4 = nn.Conv2d(10,10,3)\n",
        "        self.pool4 = nn.MaxPool2d(2,2)\n",
        "\n",
        "        self.norm = nn.BatchNorm2d(10)\n",
        "\n",
        "        self.fc1 = nn.Linear(23040,1024) #mat1 and mat2 shapes cannot be multiplied (128x23040 and 3114x1024)\n",
        "        self.fc2 = nn.Linear(1024,7)\n",
        "\n",
        "        self.localization = nn.Sequential(\n",
        "            nn.Conv2d(1, 8, kernel_size=7),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True),\n",
        "            nn.Conv2d(8, 10, kernel_size=5),\n",
        "            nn.MaxPool2d(2, stride=2),\n",
        "            nn.ReLU(True)\n",
        "        )\n",
        "\n",
        "        self.fc_loc = nn.Sequential(\n",
        "            nn.Linear(640, 32),\n",
        "            nn.ReLU(True),\n",
        "            nn.Linear(32, 3 * 2)\n",
        "        )\n",
        "        self.fc_loc[2].weight.data.zero_()\n",
        "        self.fc_loc[2].bias.data.copy_(torch.tensor([1, 0, 0, 0, 1, 0], dtype=torch.float))\n",
        "\n",
        "    def stn(self, x):\n",
        "        xs = self.localization(x)\n",
        "        xs = xs.view(-1, 640)\n",
        "        theta = self.fc_loc(xs)\n",
        "        theta = theta.view(-1, 2, 3)\n",
        "\n",
        "        grid = F.affine_grid(theta, x.size())\n",
        "        x = F.grid_sample(x, grid)\n",
        "        return x\n",
        "\n",
        "    def forward(self,input):\n",
        "        x1 = self.stn(input)\n",
        "\n",
        "        x2 = F.relu(self.conv1(input))\n",
        "        x2 = self.conv2(x2)\n",
        "        x2 = F.relu(self.pool2(x2))\n",
        "\n",
        "        x2 = F.relu(self.conv3(x2))\n",
        "        x2 = self.norm(self.conv4(x2))\n",
        "        x2 = F.relu(self.pool4(x2))\n",
        "\n",
        "        x2 = F.dropout(x2)\n",
        "        x2 = F.interpolate(x2, size=(48,48), mode='linear')\n",
        "        out = F.att(x1,x2)\n",
        "        out = torch.flatten(out, start_dim=1)\n",
        "        \n",
        "        #x2 = x2.view(-1, 810)\n",
        "        \n",
        "        #x1 = torch.flatten(x1, start_dim=1)\n",
        "        \n",
        "        \n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = self.fc2(out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "iv0E7FS-JuXH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "irJWII8iJuTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Umer2021(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Umer2021,self).__init__()\n",
        "        self.nn = nn.Sequential(\n",
        "            # Bllock 1\n",
        "            nn.Conv2d(1,64,3), # nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1),\n",
        "            nn.BatchNorm2d(64),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Dropout(),\n",
        "            # Block 2\n",
        "            nn.Conv2d(64,128,5),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Dropout(),\n",
        "            # Block 3\n",
        "            nn.Conv2d(128,128,5),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.ReLU(True),\n",
        "            nn.MaxPool2d(2,2),\n",
        "            nn.Dropout(),\n",
        "            # Block 4\n",
        "            # nn.Conv2d(128,512,3),\n",
        "            # nn.BatchNorm2d(128),\n",
        "            # nn.ReLU(True),\n",
        "            # nn.MaxPool2d(2,2),\n",
        "            # nn.Dropout(),\n",
        "            # # Block 5\n",
        "            # nn.Conv2d(512, 512, 3),\n",
        "            # nn.BatchNorm2d(128),\n",
        "            # nn.ReLU(True),\n",
        "            # nn.MaxPool2d(2, 2),\n",
        "            # nn.Dropout(),\n",
        "            # # Block 6\n",
        "            # nn.Conv2d(512, 1024, 3),\n",
        "            # nn.BatchNorm2d(128),\n",
        "            # nn.ReLU(True),\n",
        "            # nn.MaxPool2d(2, 2),\n",
        "            # nn.Dropout(),\n",
        "\n",
        "\n",
        "            # Flat layers\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(512,256),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(256, 512),\n",
        "            nn.BatchNorm2d(512),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(512,7)\n",
        "        )\n",
        "    \n",
        "    def forward(self,input):\n",
        "        x = self.nn(input)\n",
        "        return x"
      ],
      "metadata": {
        "id": "Ju74JDSVBkZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F8tGgdjjrirz"
      },
      "source": [
        "# reading data and definitions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fZj7Jrk8Y0kP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, X, Y, trans = None):\n",
        "        assert len(X) == len(Y), \"X and Y must have same size, make sure you're using the right pair\"\n",
        "        self.X = X\n",
        "        self.Y = Y\n",
        "        self.transform = trans\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "    def __getitem__(self, index):\n",
        "        image = self.X[index]\n",
        "        label = self.Y[index] #'Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral'#\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daNs3b_MQ1YJ"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "import argparse\n",
        "import numpy  as np\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.utils.data.sampler import SubsetRandomSampler\n",
        "from torchvision import transforms\n",
        "\n",
        "#from data_loaders import Plain_Dataset, eval_data_dataloader\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "from PIL import Image\n",
        "\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/dataset/fer2013.csv\")\n",
        "data.head()\n",
        "\n",
        "groups = [g for _, g in data.groupby('Usage')]\n",
        "# print([x for x,_ in data.groupby('Usage')])\n",
        "train = groups[2]\n",
        "val = groups[1]\n",
        "test = groups[0]\n",
        "train = train.drop(labels=['Usage'], axis=1)\n",
        "val = val.drop(labels=['Usage'], axis=1)\n",
        "test = test.drop(labels=['Usage'], axis=1)\n",
        "\n",
        "Y_train = train[\"emotion\"]\n",
        "Y_val = val[\"emotion\"]\n",
        "Y_test = test[\"emotion\"]\n",
        "\n",
        "\n",
        "X_train = train[\"pixels\"]\n",
        "X_val = val[\"pixels\"]\n",
        "X_test = test[\"pixels\"]\n",
        "\n",
        "def preprocess(X):\n",
        "    X = np.array([np.fromstring(image, np.uint8, sep=' ') for image in X]).astype(np.float32)\n",
        "    X = X/255.0\n",
        "    X = X.reshape(-1, 48, 48, 1)\n",
        "    return X\n",
        "\n",
        "X_train = preprocess(X_train)\n",
        "X_val = preprocess(X_val)\n",
        "X_test = preprocess(X_test)\n",
        "Y_train = np.array(Y_train)\n",
        "Y_val = np.array(Y_val)\n",
        "Y_test = np.array(Y_test)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transforms"
      ],
      "metadata": {
        "id": "UeycyH2rVjbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torchvision.transforms as T\n",
        "# Data transforms (Gray Scaling & data augmentation)\n",
        "train_transform = T.Compose([T.ToTensor(), T.Normalize((0.5,),(0.5,)), T.RandomHorizontalFlip(),\n",
        "                             T.RandomAdjustSharpness(0.9, p=0), T.RandomRotation(5)])#, tt.RandomRotation(10)])\n",
        "transform =  T.Compose([T.ToTensor(),T.Normalize((0.5,),(0.5,))])\n",
        "\n",
        "# ------------------------------------------------------------------\n",
        "train_dataset = CustomDataset(X_train, Y_train, train_transform)\n",
        "test_dataset = CustomDataset(X_test, Y_test, transform)\n",
        "val_dataset = CustomDataset(X_val, Y_val, transform)"
      ],
      "metadata": {
        "id": "-xmfXMaGVSFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abf3qE9lifLH"
      },
      "source": [
        "### train function\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XolyBtaCy4t-"
      },
      "outputs": [],
      "source": [
        "def Train(net,epochs,train_loader,val_loader,criterion,optmizer,device, writer):\n",
        "    '''\n",
        "    Training Loop\n",
        "    '''\n",
        "    print(\"===================================Start Training===================================\")\n",
        "    for e in range(epochs):\n",
        "        train_loss = 0\n",
        "        validation_loss = 0\n",
        "        train_correct = 0\n",
        "        val_correct = 0\n",
        "        # Train the model  #\n",
        "        net.train()\n",
        "        for i, (data, labels) in enumerate(train_loader):\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            optmizer.zero_grad()\n",
        "            outputs = net(data)\n",
        "            loss = criterion(outputs,labels)\n",
        "            loss.backward()\n",
        "            optmizer.step()\n",
        "            train_loss += loss.item()\n",
        "            _, preds = torch.max(outputs,1)\n",
        "            train_correct += torch.sum(preds == labels.data)\n",
        "            print('\\r', 'Epoch: %d | score : %.2f' \\\n",
        "                %(e+1, train_correct/(i+0.0001)), end = '') \n",
        "        print('\\r', end='')\n",
        "        #validate the model#\n",
        "        net.eval()\n",
        "        for data,labels in val_loader:\n",
        "            data, labels = data.to(device), labels.to(device)\n",
        "            val_outputs = net(data)\n",
        "            val_loss = criterion(val_outputs, labels)\n",
        "            validation_loss += val_loss.item()\n",
        "            _, val_preds = torch.max(val_outputs,1)\n",
        "            val_correct += torch.sum(val_preds == labels.data)\n",
        "            \n",
        "\n",
        "        train_loss = train_loss/len(train_dataset)\n",
        "        train_acc = train_correct.double() / len(train_dataset)\n",
        "        validation_loss =  validation_loss / len(val_dataset)\n",
        "        val_acc = val_correct.double() / len(val_dataset)\n",
        "        print('Epoch: {} \\tTraining Loss: {:.8f} \\tValidation Loss {:.8f} \\tTraining Acuuarcy {:.3f}% \\tValidation Acuuarcy {:.3f}%'\n",
        "                                                           .format(e+1, train_loss,validation_loss,train_acc * 100, val_acc*100))\n",
        "        writer.add_scalar(\"Loss/train\", train_loss, e)\n",
        "        writer.add_scalar(\"Accuracy/train\", train_acc, e)\n",
        "        writer.add_scalar(\"Loss/val\", validation_loss, e)\n",
        "        writer.add_scalar(\"Accuracy/val\", val_acc, e)\n",
        "\n",
        "    torch.save(net.state_dict(),'deep_emotion-{}-{}-{}.pt'.format(epochs,batch_size,lr))\n",
        "    print(\"===================================Training Finished===================================\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WUUcEW5aLp5n"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckQm6WOgL2P2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 592
        },
        "outputId": "f6b36468-81d8-4a9d-f10f-c61252246721"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "===================================Start Training===================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4066: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:4004: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
            "  \"Default grid_sample and affine_grid behavior has changed \"\n",
            "/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py:3635: UserWarning: Default upsampling behavior when mode=linear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
            "  \"See the documentation of nn.Upsample for details.\".format(mode)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NotImplementedError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-38-1646bcde35a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0moptmizer\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0mTrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptmizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwriter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-15-962833e95587>\u001b[0m in \u001b[0;36mTrain\u001b[0;34m(net, epochs, train_loader, val_loader, criterion, optmizer, device, writer)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0moptmizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-f0ec7350fe0f>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m         \u001b[0mx2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minterpolate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'linear'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36minterpolate\u001b[0;34m(input, size, scale_factor, mode, align_corners, recompute_scale_factor)\u001b[0m\n\u001b[1;32m   3742\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got 3D input, but trilinear mode needs 5D input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3743\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"linear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3744\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got 4D input, but linear mode needs 3D input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3745\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"trilinear\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3746\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Got 4D input, but trilinear mode needs 5D input\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNotImplementedError\u001b[0m: Got 4D input, but linear mode needs 3D input"
          ]
        }
      ],
      "source": [
        "# hyperparams\n",
        "epochs = 100\n",
        "lr = 0.005\n",
        "use_class_weights = False\n",
        "batch_size = 128\n",
        "# network\n",
        "net = Deep_Emotion_with_mult()\n",
        "\n",
        "# PyTorch data loaders\n",
        "train_loader = DataLoader(train_dataset, batch_size, num_workers=2, pin_memory=True, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size, num_workers=2, pin_memory=True)\n",
        "test_loader  = DataLoader(test_dataset,  batch_size, num_workers=2, pin_memory=True)\n",
        "\n",
        "\n",
        "\n",
        "import gc\n",
        "gc.collect()\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net.to(device)\n",
        "traincsv_file = '/content/datasets/train.csv'\n",
        "validationcsv_file = '/content/datasets/val.csv'\n",
        "train_img_dir = '/content/datasets/train/'\n",
        "validation_img_dir = '/content/datasets/val/'\n",
        "\n",
        "transformation= transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.5,),(0.5,))])\n",
        "#train_dataset= Plain_Dataset(csv_file=traincsv_file, img_dir = train_img_dir, datatype = 'train', transform = transformation)\n",
        "#validation_dataset= Plain_Dataset(csv_file=validationcsv_file, img_dir = validation_img_dir, datatype = 'val', transform = transformation)\n",
        "#train_loader= DataLoader(train_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
        "#val_loader=   DataLoader(validation_dataset,batch_size=batchsize,shuffle = True,num_workers=0)\n",
        "writer = SummaryWriter('runs/fer2013_experiment_1')\n",
        "cweights = [1.02660468, 9.40661861, 1.00104606, 0.56843877, 0.84912748, 1.29337298, 0.82603942]\n",
        "class_weights = torch.FloatTensor(cweights).cuda()\n",
        "if use_class_weights:\n",
        "    criterion= nn.CrossEntropyLoss(weight=class_weights)\n",
        "else:\n",
        "    criterion= nn.CrossEntropyLoss()\n",
        "optmizer= optim.Adam(net.parameters(),lr= lr)\n",
        "Train(net,epochs, train_loader, val_loader, criterion, optmizer, device, writer)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net, \"/content/drive/MyDrive/Colab Notebooks/Face emotion detection/Models/tmp13548\")"
      ],
      "metadata": {
        "id": "cq6UrfmJbFpZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"latest execution : concat model\n",
        "\"Epoch: 88 \tTraining Loss: 0.00890384 \tValidation Loss 0.01008887 \tTraining Acuuarcy 56.484% \tValidation Acuuarcy 52.327%\n",
        "Epoch: 89 \tTraining Loss: 0.00892394 \tValidation Loss 0.00995875 \tTraining Acuuarcy 56.703% \tValidation Acuuarcy 53.246%\n",
        "Epoch: 90 \tTraining Loss: 0.00888090 \tValidation Loss 0.01016509 \tTraining Acuuarcy 56.906% \tValidation Acuuarcy 53.608%\n",
        "Epoch: 91 \tTraining Loss: 0.00891105 \tValidation Loss 0.01013295 \tTraining Acuuarcy 57.090% \tValidation Acuuarcy 52.522%\n",
        "Epoch: 92 \tTraining Loss: 0.00888799 \tValidation Loss 0.01044168 \tTraining Acuuarcy 56.832% \tValidation Acuuarcy 52.271%\n",
        "Epoch: 93 \tTraining Loss: 0.00889699 \tValidation Loss 0.01008596 \tTraining Acuuarcy 56.919% \tValidation Acuuarcy 53.190%\n",
        "Epoch: 94 \tTraining Loss: 0.00888363 \tValidation Loss 0.01012353 \tTraining Acuuarcy 56.860% \tValidation Acuuarcy 53.302%\n",
        "Epoch: 95 \tTraining Loss: 0.00882737 \tValidation Loss 0.01007196 \tTraining Acuuarcy 57.027% \tValidation Acuuarcy 53.469%\n",
        "Epoch: 96 \tTraining Loss: 0.00886491 \tValidation Loss 0.00996131 \tTraining Acuuarcy 56.735% \tValidation Acuuarcy 52.856%\n",
        "Epoch: 97 \tTraining Loss: 0.00886380 \tValidation Loss 0.00995262 \tTraining Acuuarcy 57.041% \tValidation Acuuarcy 53.775%\n",
        "Epoch: 98 \tTraining Loss: 0.00886352 \tValidation Loss 0.00999040 \tTraining Acuuarcy 57.362% \tValidation Acuuarcy 54.166%\n",
        "Epoch: 99 \tTraining Loss: 0.00880713 \tValidation Loss 0.01013636 \tTraining Acuuarcy 57.135% \tValidation Acuuarcy 52.967%\n",
        "Epoch: 100 \tTraining Loss: 0.00889800 \tValidation Loss 0.01013484 \tTraining Acuuarcy 56.951% \tValidation Acuuarcy 52.995%\n",
        "===================================Training Finished===================================\"\"\""
      ],
      "metadata": {
        "id": "05Gpyn0BjUiZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "test 2022 - certitude.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}